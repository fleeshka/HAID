services:
  ollama:
    image: ollama/ollama:latest 
    container_name: ollama
    ports:
      - "11434:11434"  
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    networks:
      - app-network
    mem_limit: 3g
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              capabilities: [gpu]
#    environment:
#      - NVIDIA_VISIBLE_DEVICES=all
#    runtime: nvidia
    mem_reservation: 2g
    cpus: 1.5
    environment:
      - OLLAMA_NO_CUDA=1
      - OLLAMA_KEEP_ALIVE=10m
      - OLLAMA_MAX_LOADED_MODELS=

    entrypoint: ["/bin/bash", "-c", "ollama serve & sleep 10 && ollama run tinyllama && tail -f /dev/null"]
    healthcheck:
      test: curl --fail http://ollama:11434/api/tags || exit 1
      interval: 15s
      timeout: 5s
      retries: 30


  bot:
    build:
      context: . 
      dockerfile: Dockerfile
    container_name: telegram-bot
    depends_on:
      ollama:
        condition: service_healthy
    env_file:
      - .env  
    volumes:
      - ./data:/app/data
      - ./src:/app/src
    restart: unless-stopped
    networks:
      - app-network
    mem_limit: 2g
    cpus: 1

networks:
  app-network:
    driver: bridge

volumes:
  ollama-data: 
